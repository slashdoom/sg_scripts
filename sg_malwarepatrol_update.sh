#!/bin/sh
#
# shalla_update.sh, v 0.3.1 20080403
# done by kapivie at sil.at under FreeBSD
# without any warranty
# updated by Len Tucker to create and use diff
# files to reduce load and increase speed.
# Added Checks for required elements
# Added output info for status of script
# Modified by Chris Kronberg: included loop; added some more
# checks; reduced the diff files to the necessary content.
# Modified by Patrick Ryon (slashdoom): reconfigured for
# MalwarePatrol lists and ufdbGuard.
#
#--------------------------------------------------
# little script (for crond)
# to fetch and modify new list from shallalist.de
#--------------------------------------------------
#
# *check* paths and squidGuard-owner on your system
# try i.e. "which squid" to find out the path for squid
# try "ps aux | grep squid" to find out the owner for squidGuard
#     *needs wget*
#

squidpath  = "/usr/sbin/squid3"
tarpath    = "/bin/tar"
chownpath  = "/bin/chown"
httpget    = "/usr/bin/wget"
wgetlogdir = "/usr/local/ufdbguard/logs"

shallalist = "https://lists.malwarepatrol.net/cgi/getfile?receipt=f1395928453&product=8&list=dansguardian"

dbhome     = "/usr/local/ufdbguard/blacklists/malware"     # like in squidGuard.conf
squidGuardowner = "proxy:proxy"

##########################################

workdir    = "/usr/local/ufdbguard/tmp"

if [ ! -d $workdir ]; then
  mkdir -p $workdir
fi

if [ ! -f $tarpath ]
 then echo "Could not locate tar."
      exit 1
fi

if [ ! -f $chownpath ]
 then echo "Could not locate chown."
      exit 1
fi

if [ ! -d  $dbhome ]
 then echo "Could not locate squid db directory."
      exit 1
fi

# check that everything is clean before we start.
if [ -f  $workdir/urls ]; then
   echo "Old blacklist file found in ${workdir}. Deleted!"
   rm $workdir/urls
fi

# copy actual shalla's blacklist
# thanks for the " || exit 1 " hint to Rich Wales
# (-b run in background does not work correctly) -o log to $wgetlog

TODAY=$(date)
HOST=$(hostname)
echo "-----------------------------------------------------"
echo "Date: $TODAY          Host:$HOST"
echo "-----------------------------------------------------"

echo "Retrieving MalwarePatrol.net blacklist"

$httpget $shallalist -a $wgetlogdir/sg_malwarepatrol_wget.log -O $workdir/urls || { echo "Unable to download malware patrol URLs." && exit 1 ; }

# Create diff files for all categories
# Note: There is no reason to use all categories unless this is exactly
#       what you intend to block. Make sure that only the categories you
#       are going to block with squidGuard are listed below.

#CATEGORIES="adv aggressive automobile/cars automobile/bikes automobile/planes automobile/boats chat dating downloads drugs dynamic finance/banking finance/insurance finance/other finance/moneylending finance/realestate forum gamble hacking hobby/cooking hobby/games hobby/pets hospitals imagehosting isp jobsearch models movies music news podcasts politcs porn recreation/humor recreation/sports recreation/travel recreation/wellness redirector religion ringtones science/astronomy science/chemistry searchengines sex/lingerie shopping socialnet spyware tracker updatesites violence warez weapons webmail webphone webradio webtv" 

echo "Creating diff files."
# The "cp" after the "diff" ensures that we keep up to date with our 
# domains and urls files.

if [ -f $workdir/urls ] && [ -f $dbhome/urls ]
  then
    diff -ur $dbhome/urls $workdir/urls > $dbhome/urls.diff
    cp $workdir/urls $dbhome/urls
  else
    cp $workdir/urls $dbhome/urls
fi

echo "Setting file permisions."
$chownpath -R $squidGuardowner $dbhome
chmod 755 $dbhome
cd $dbhome
find . -type f -exec chmod 644 {} \;
find . -type d -exec chmod 755 {} \;

echo "Reconfiguring squid."
$squidpath -k reconfigure

echo "Clean up downloaded file and directories."
rm $workdir/urls

exit 0
